{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple NB Based Lingspam Spam classifier \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets as skd\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "#Each sub-directory in the parent directory is assumed to contain documents from the same class\n",
    "#I pre-processed the part1 (fold1) and part2 (fold2) of the lingspam dataset to place spam emails in one folder \n",
    "#and legit emails in another; you should do the same for the entire dataset, either manually or via a script. \n",
    "ls_train = skd.load_files('./data/lingspam_public/lemm_stop/train');\n",
    "ls_test  = skd.load_files('./data/lingspam_public/lemm_stop/test');\n",
    "\n",
    "#The count vectorizer classes fit_transform function generates a vocoabulary that contains each unique term in the dataset\n",
    "#and outputs a sparse matrix tabulating term occurences\n",
    "count_vect = CountVectorizer()\n",
    "x_train = count_vect.fit_transform(ls_train.data)\n",
    "\n",
    "#Since the vocabulary has already been learned, use the transform function to transform the test data using the same vocab\n",
    "x_test = count_vect.transform(ls_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(N,x_train):\n",
    "    # Prepare\n",
    "    num_email   = x_train.shape[0]\n",
    "    num_feature = x_train.shape[1]\n",
    "\n",
    "    # Transport saved data from sparse matrix out\n",
    "    x_train_data = x_train.toarray()\n",
    "    x_train_ig   = np.zeros([num_feature])\n",
    "\n",
    "    for i in range(num_feature):\n",
    "        # Each colunm show the occurence of one feature in all emails\n",
    "        feature_vector = x_train_data[:,i]\n",
    "    \n",
    "        # Reshape \n",
    "        feature_vector = feature_vector.reshape([num_email])\n",
    "        \n",
    "        # Calculate ig for features \n",
    "        x_train_ig [i] = mutual_info_score(feature_vector, ls_train.target)\n",
    "        \n",
    "    x_train_ig_sort = np.argsort(-x_train_ig)\n",
    "\n",
    "    # Extract feature names\n",
    "    name_feature = count_vect.get_feature_names()\n",
    "\n",
    "    # Select N largest features' index\n",
    "    top_feature  = np.array(x_train_ig_sort[:N])\n",
    "    drop_feature = np.array(x_train_ig_sort[N:num_feature])\n",
    "    \n",
    "    return top_feature,drop_feature,name_feature\n",
    "\n",
    "def print_feature(top_feature,name_feature):\n",
    "    for i in range(len(top_feature)):\n",
    "        print(name_feature[top_feature[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "free\n",
      "remove\n",
      "linguistic\n",
      "university\n",
      "money\n",
      "our\n",
      "click\n",
      "business\n",
      "market\n",
      "\n",
      "\n",
      "language\n",
      "free\n",
      "remove\n",
      "linguistic\n",
      "university\n",
      "money\n",
      "our\n",
      "click\n",
      "business\n",
      "market\n",
      "today\n",
      "internet\n",
      "over\n",
      "check\n",
      "product\n",
      "order\n",
      "sell\n",
      "advertise\n",
      "company\n",
      "million\n",
      "100\n",
      "day\n",
      "want\n",
      "english\n",
      "easy\n",
      "best\n",
      "income\n",
      "linguistics\n",
      "save\n",
      "every\n",
      "receive\n",
      "guarantee\n",
      "thousand\n",
      "service\n",
      "bulk\n",
      "mail\n",
      "com\n",
      "buy\n",
      "cash\n",
      "purchase\n",
      "ll\n",
      "cost\n",
      "win\n",
      "edu\n",
      "start\n",
      "dollar\n",
      "address\n",
      "fax\n",
      "mailing\n",
      "offer\n",
      "yourself\n",
      "list\n",
      "papers\n",
      "month\n",
      "hour\n",
      "earn\n",
      "hundred\n",
      "linguist\n",
      "live\n",
      "success\n",
      "here\n",
      "week\n",
      "20\n",
      "theory\n",
      "pay\n",
      "email\n",
      "conference\n",
      "credit\n",
      "ever\n",
      "customer\n",
      "card\n",
      "send\n",
      "profit\n",
      "abstract\n",
      "yours\n",
      "financial\n",
      "need\n",
      "fun\n",
      "speaker\n",
      "watch\n",
      "home\n",
      "name\n",
      "sale\n",
      "bonus\n",
      "zip\n",
      "discussion\n",
      "toll\n",
      "instruction\n",
      "simply\n",
      "syntax\n",
      "amaze\n",
      "online\n",
      "anywhere\n",
      "investment\n",
      "off\n",
      "site\n",
      "department\n",
      "ad\n",
      "program\n",
      "friend\n",
      "\n",
      "\n",
      "language\n",
      "free\n",
      "remove\n",
      "linguistic\n",
      "university\n",
      "money\n",
      "our\n",
      "click\n",
      "business\n",
      "market\n",
      "today\n",
      "internet\n",
      "over\n",
      "check\n",
      "product\n",
      "order\n",
      "sell\n",
      "advertise\n",
      "company\n",
      "million\n",
      "100\n",
      "day\n",
      "want\n",
      "english\n",
      "easy\n",
      "best\n",
      "income\n",
      "linguistics\n",
      "save\n",
      "every\n",
      "receive\n",
      "guarantee\n",
      "thousand\n",
      "service\n",
      "bulk\n",
      "mail\n",
      "com\n",
      "buy\n",
      "cash\n",
      "purchase\n",
      "ll\n",
      "cost\n",
      "win\n",
      "edu\n",
      "start\n",
      "dollar\n",
      "address\n",
      "fax\n",
      "mailing\n",
      "offer\n",
      "yourself\n",
      "list\n",
      "papers\n",
      "month\n",
      "hour\n",
      "earn\n",
      "hundred\n",
      "linguist\n",
      "live\n",
      "success\n",
      "here\n",
      "week\n",
      "20\n",
      "theory\n",
      "pay\n",
      "email\n",
      "conference\n",
      "credit\n",
      "ever\n",
      "customer\n",
      "card\n",
      "send\n",
      "profit\n",
      "abstract\n",
      "yours\n",
      "financial\n",
      "need\n",
      "fun\n",
      "speaker\n",
      "watch\n",
      "home\n",
      "name\n",
      "sale\n",
      "bonus\n",
      "zip\n",
      "discussion\n",
      "toll\n",
      "instruction\n",
      "simply\n",
      "syntax\n",
      "amaze\n",
      "online\n",
      "anywhere\n",
      "investment\n",
      "off\n",
      "site\n",
      "department\n",
      "ad\n",
      "program\n",
      "friend\n",
      "grammar\n",
      "ship\n",
      "huge\n",
      "line\n",
      "dream\n",
      "science\n",
      "back\n",
      "study\n",
      "structure\n",
      "cd\n",
      "de\n",
      "wait\n",
      "deadline\n",
      "report\n",
      "opportunity\n",
      "us\n",
      "step\n",
      "next\n",
      "again\n",
      "package\n",
      "legal\n",
      "mlm\n",
      "research\n",
      "keep\n",
      "security\n",
      "analysis\n",
      "try\n",
      "workshop\n",
      "fresh\n",
      "once\n",
      "down\n",
      "spend\n",
      "topic\n",
      "xxx\n",
      "marketing\n",
      "price\n",
      "amount\n",
      "net\n",
      "secret\n",
      "adult\n",
      "each\n",
      "issue\n",
      "profitable\n",
      "immediately\n",
      "right\n",
      "add\n",
      "fantastic\n",
      "advertisement\n",
      "never\n",
      "risk\n",
      "delete\n",
      "freedom\n",
      "own\n",
      "debt\n",
      "reference\n",
      "simple\n",
      "overnight\n",
      "excite\n",
      "aol\n",
      "message\n",
      "semantic\n",
      "hello\n",
      "discourse\n",
      "join\n",
      "software\n",
      "great\n",
      "committee\n",
      "even\n",
      "bank\n",
      "reply\n",
      "everything\n",
      "delivery\n",
      "development\n",
      "1998\n",
      "orders\n",
      "ac\n",
      "author\n",
      "exactly\n",
      "theoretical\n",
      "generate\n",
      "submission\n",
      "spam\n",
      "sex\n",
      "sure\n",
      "search\n",
      "per\n",
      "nothing\n",
      "speech\n",
      "decide\n",
      "24\n",
      "bill\n",
      "web\n",
      "enter\n",
      "return\n",
      "resell\n",
      "fill\n",
      "hit\n",
      "unlimit\n",
      "is\n",
      "ve\n",
      "hot\n",
      "life\n",
      "much\n",
      "personal\n",
      "paper\n",
      "50\n",
      "888\n",
      "tax\n",
      "sales\n",
      "partner\n",
      "visa\n",
      "stealth\n",
      "tel\n",
      "acquisition\n",
      "letter\n",
      "800\n",
      "work\n",
      "help\n",
      "campaign\n",
      "tell\n",
      "everyone\n",
      "capital\n",
      "re\n",
      "phonology\n",
      "love\n",
      "most\n",
      "absolutely\n",
      "computational\n",
      "hottest\n",
      "video\n",
      "lexical\n",
      "between\n",
      "remember\n",
      "student\n",
      "little\n",
      "big\n",
      "many\n",
      "effective\n",
      "phone\n",
      "aspect\n",
      "total\n",
      "envelope\n",
      "choose\n",
      "don\n",
      "cognitive\n",
      "95\n",
      "engine\n",
      "put\n",
      "undeliverable\n",
      "making\n",
      "amazing\n",
      "plans\n",
      "context\n",
      "monthly\n",
      "always\n",
      "1995\n",
      "read\n",
      "semantics\n",
      "top\n",
      "word\n",
      "works\n",
      "invest\n",
      "500\n",
      "below\n",
      "please\n",
      "lottery\n",
      "guaranteed\n",
      "window\n",
      "visit\n",
      "worldwide\n",
      "future\n",
      "french\n",
      "office\n",
      "car\n",
      "200\n",
      "rate\n",
      "refund\n",
      "print\n",
      "reports\n",
      "luck\n",
      "really\n",
      "prove\n",
      "federal\n",
      "lose\n",
      "brand\n",
      "relax\n",
      "syntactic\n",
      "fortune\n",
      "already\n",
      "clean\n",
      "john\n",
      "chance\n",
      "mastercard\n",
      "weekly\n",
      "summary\n",
      "institute\n",
      "focus\n",
      "affiliation\n",
      "follow\n",
      "verb\n",
      "recruit\n",
      "book\n",
      "millions\n",
      "hundreds\n",
      "hobby\n",
      "extra\n",
      "construction\n",
      "morphology\n",
      "move\n",
      "four\n",
      "cent\n",
      "seven\n",
      "within\n",
      "duplicate\n",
      "faster\n",
      "retire\n",
      "participate\n",
      "charge\n",
      "german\n",
      "translation\n",
      "level\n",
      "multi\n",
      "let\n",
      "corporations\n",
      "dollars\n",
      "session\n",
      "doubt\n",
      "comply\n",
      "powerful\n",
      "lists\n",
      "super\n",
      "advantage\n",
      "city\n",
      "april\n",
      "better\n",
      "create\n",
      "quick\n",
      "invite\n",
      "enjoy\n",
      "role\n",
      "pragmatic\n",
      "totally\n",
      "show\n",
      "practically\n",
      "pp\n",
      "yahoo\n",
      "plus\n",
      "competition\n",
      "suite\n",
      "1302\n",
      "gamble\n",
      "hesitate\n",
      "sources\n",
      "query\n",
      "miss\n",
      "greatest\n",
      "excess\n",
      "perspective\n",
      "chair\n",
      "true\n",
      "sender\n",
      "completely\n",
      "are\n",
      "lot\n",
      "european\n",
      "alter\n",
      "game\n",
      "owe\n",
      "billion\n",
      "id\n",
      "paste\n",
      "historical\n",
      "make\n",
      "researcher\n",
      "corpus\n",
      "box\n",
      "mailer\n",
      "10\n",
      "bankruptcy\n",
      "after\n",
      "teen\n",
      "rom\n",
      "550\n",
      "grammatical\n",
      "evidence\n",
      "speak\n",
      "everythe\n",
      "member\n",
      "submit\n",
      "august\n",
      "away\n",
      "mailbox\n",
      "general\n",
      "programme\n",
      "legitimate\n",
      "registration\n",
      "presentation\n",
      "sentence\n",
      "modern\n",
      "sincerely\n",
      "until\n",
      "code\n",
      "approach\n",
      "expiration\n",
      "two\n",
      "same\n",
      "lucky\n",
      "trade\n",
      "bottom\n",
      "programs\n",
      "ye\n",
      "those\n",
      "millionaire\n",
      "downline\n",
      "tips\n",
      "native\n",
      "family\n",
      "evaluating\n",
      "literature\n",
      "eliminate\n",
      "forever\n",
      "wrap\n",
      "release\n",
      "formal\n",
      "truly\n",
      "quit\n",
      "filter\n",
      "discover\n",
      "nl\n",
      "run\n",
      "afford\n",
      "before\n",
      "junk\n",
      "aim\n",
      "discuss\n",
      "constraint\n",
      "secure\n",
      "beach\n",
      "pick\n",
      "interaction\n",
      "accurately\n",
      "mci\n",
      "1341\n",
      "mortgage\n",
      "intrusion\n",
      "scam\n",
      "vacation\n",
      "deposit\n",
      "call\n",
      "datum\n",
      "stop\n",
      "allow\n",
      "file\n",
      "unsubscribe\n",
      "http\n",
      "addresses\n",
      "representation\n",
      "industry\n",
      "present\n",
      "incredible\n",
      "signature\n",
      "latest\n",
      "services\n",
      "buyer\n",
      "one\n",
      "pass\n",
      "roll\n",
      "honest\n",
      "girl\n",
      "easiest\n",
      "uk\n",
      "society\n",
      "second\n",
      "ticket\n",
      "academic\n",
      "else\n",
      "law\n",
      "real\n",
      "through\n",
      "relevant\n",
      "imagine\n",
      "dialect\n",
      "organize\n",
      "easily\n",
      "ez\n",
      "worth\n",
      "response\n",
      "250\n",
      "succeed\n",
      "effort\n",
      "movie\n",
      "notification\n",
      "cleanest\n",
      "earnings\n",
      "instant\n",
      "capitalfm\n",
      "fabulous\n",
      "overload\n",
      "shop\n",
      "wish\n",
      "announcement\n",
      "server\n",
      "thing\n",
      "download\n",
      "tip\n",
      "culture\n",
      "description\n",
      "ed\n",
      "testimonial\n",
      "fast\n",
      "modem\n",
      "sit\n",
      "gold\n",
      "instructions\n",
      "theme\n",
      "piece\n",
      "verify\n",
      "anyone\n",
      "someone\n",
      "germany\n",
      "under\n",
      "number\n",
      "enterprise\n",
      "speed\n",
      "variety\n",
      "trial\n",
      "forget\n",
      "sociolinguistic\n",
      "phonological\n",
      "relation\n",
      "protect\n",
      "hours\n",
      "released\n",
      "staggering\n",
      "creditor\n",
      "reg\n",
      "sexually\n",
      "totals\n",
      "numbers\n",
      "cram\n",
      "astonishment\n",
      "publication\n",
      "type\n",
      "lexicon\n",
      "perfectly\n",
      "place\n",
      "isp\n",
      "proof\n",
      "argument\n",
      "subject\n",
      "automatically\n",
      "sexual\n",
      "yes\n",
      "less\n",
      "did\n",
      "leave\n",
      "financially\n",
      "pattern\n",
      "31\n",
      "believe\n",
      "comparative\n",
      "vanish\n",
      "few\n",
      "plan\n",
      "interpretation\n",
      "particular\n",
      "earth\n",
      "marketer\n",
      "natural\n",
      "laugh\n",
      "prompt\n",
      "provider\n",
      "alway\n",
      "magazine\n",
      "protection\n",
      "waste\n",
      "started\n",
      "contribution\n",
      "framework\n",
      "limited\n",
      "accountant\n",
      "raleigh\n",
      "believer\n",
      "entrepreneur\n",
      "anytime\n",
      "estate\n",
      "unlimited\n",
      "juno\n",
      "grumble\n",
      "spokane\n",
      "merciless\n",
      "desirous\n",
      "spout\n",
      "fairchild\n",
      "webmaster\n",
      "break\n",
      "upgrade\n",
      "van\n",
      "90\n",
      "robert\n",
      "rights\n",
      "3d\n",
      "cambridge\n",
      "state\n",
      "additional\n",
      "instruct\n",
      "conceal\n",
      "ordering\n",
      "dori\n",
      "computer\n",
      "300\n",
      "professor\n",
      "thank\n",
      "france\n",
      "proceedings\n",
      "publish\n",
      "introduction\n",
      "least\n",
      "cultural\n",
      "news\n",
      "bel\n",
      "entire\n",
      "spanish\n",
      "chat\n",
      "noun\n",
      "et\n",
      "corporation\n",
      "hardcore\n",
      "access\n",
      "air\n",
      "world\n",
      "clearance\n",
      "professional\n",
      "mit\n",
      "complex\n",
      "alone\n",
      "batch\n",
      "99\n",
      "genie\n",
      "lawful\n",
      "rockland\n",
      "stun\n",
      "esq\n",
      "weeks\n",
      "dupe\n",
      "qualify\n",
      "application\n",
      "degree\n",
      "request\n",
      "color\n",
      "cross\n",
      "postage\n",
      "unsolicit\n",
      "phonetic\n",
      "expression\n",
      "fraction\n",
      "extraordinary\n",
      "boy\n",
      "error\n",
      "ours\n",
      "retirement\n",
      "case\n",
      "quickly\n",
      "forum\n",
      "rich\n",
      "variation\n",
      "isbn\n",
      "text\n",
      "length\n",
      "owner\n",
      "association\n",
      "am\n",
      "almost\n",
      "hand\n",
      "days\n",
      "action\n",
      "small\n",
      "among\n",
      "1994\n",
      "editor\n",
      "article\n",
      "proposal\n",
      "advertiser\n",
      "welcome\n",
      "potential\n",
      "client\n",
      "extractor\n",
      "faith\n",
      "compliance\n",
      "anything\n",
      "gift\n",
      "reach\n",
      "turn\n",
      "faculty\n",
      "removed\n",
      "3005\n",
      "awesome\n",
      "prepared\n",
      "robbery\n",
      "prodigy\n",
      "rip\n",
      "retail\n",
      "thousands\n",
      "privacy\n",
      "selle\n",
      "refinance\n",
      "blvd\n",
      "japanese\n",
      "music\n",
      "sent\n",
      "enclose\n",
      "hold\n",
      "acceptance\n",
      "radio\n",
      "record\n",
      "march\n",
      "amateur\n",
      "toy\n",
      "bet\n",
      "reap\n",
      "470\n",
      "require\n",
      "md\n",
      "payable\n",
      "penny\n",
      "revenue\n",
      "wall\n",
      "chapter\n",
      "chinese\n",
      "david\n",
      "deliver\n",
      "tremendous\n",
      "june\n",
      "school\n",
      "increase\n",
      "principle\n",
      "happen\n",
      "write\n",
      "test\n",
      "journal\n",
      "dictionary\n",
      "gov\n",
      "successful\n",
      "particularly\n",
      "trash\n",
      "au\n",
      "argue\n",
      "graduate\n",
      "400\n",
      "dissertation\n",
      "1992\n",
      "store\n",
      "button\n",
      "ram\n",
      "unique\n",
      "loan\n",
      "morphological\n",
      "notion\n",
      "camera\n",
      "affordable\n",
      "info\n",
      "complete\n",
      "centre\n",
      "savings\n",
      "whatsoever\n",
      "ling\n",
      "removal\n",
      "club\n",
      "mclaughlin\n",
      "soon\n",
      "stock\n",
      "daily\n",
      "goal\n",
      "percentage\n",
      "cyber\n",
      "parallel\n",
      "uni\n",
      "fm\n",
      "classified\n",
      "boyfriend\n",
      "secrets\n",
      "msn\n",
      "premium\n",
      "celebrity\n",
      "1618\n",
      "filled\n",
      "living\n",
      "emailer\n",
      "asset\n",
      "skeptical\n",
      "hi\n",
      "1999\n",
      "1993\n",
      "colleague\n",
      "dial\n",
      "rat\n",
      "can\n",
      "meg\n",
      "content\n",
      "inexpensive\n",
      "style\n",
      "casino\n",
      "clause\n",
      "lecture\n",
      "distinction\n",
      "bin\n",
      "shock\n",
      "dept\n",
      "30\n",
      "berlin\n",
      "why\n",
      "model\n",
      "job\n",
      "rush\n",
      "february\n",
      "600\n",
      "important\n",
      "expensive\n",
      "vium\n",
      "netherland\n",
      "lifetime\n",
      "collect\n",
      "instantly\n",
      "multus\n",
      "vowel\n",
      "logic\n",
      "wealth\n",
      "high\n",
      "700\n",
      "must\n",
      "valuable\n",
      "residual\n",
      "postal\n",
      "paradise\n",
      "woman\n",
      "mark\n",
      "communication\n",
      "assume\n",
      "process\n",
      "section\n",
      "operate\n",
      "too\n",
      "entertainment\n",
      "psycholinguistic\n",
      "scholar\n",
      "symposium\n",
      "benefits\n",
      "blast\n",
      "continue\n",
      "persistent\n",
      "able\n",
      "jackson\n",
      "hotmail\n",
      "realistic\n",
      "exclusive\n",
      "michael\n",
      "using\n",
      "cut\n",
      "organizer\n",
      "der\n",
      "richer\n",
      "goods\n",
      "confidential\n",
      "project\n",
      "inflation\n",
      "stamped\n",
      "addressed\n",
      "pile\n",
      "included\n",
      "seller\n",
      "someday\n",
      "vulgarity\n",
      "wilburn\n",
      "win95\n",
      "biz\n",
      "porn\n",
      "course\n",
      "first\n",
      "cds\n",
      "low\n",
      "nc\n",
      "digital\n",
      "currency\n",
      "recent\n",
      "extremely\n",
      "link\n",
      "phrase\n",
      "method\n",
      "sponsor\n",
      "guide\n",
      "winner\n",
      "human\n",
      "term\n",
      "password\n",
      "payment\n",
      "empirical\n",
      "campus\n",
      "holiday\n",
      "immediate\n",
      "golden\n",
      "convenience\n",
      "cable\n",
      "fl\n",
      "relate\n",
      "kid\n",
      "population\n",
      "page\n",
      "scientific\n",
      "ahead\n",
      "jump\n",
      "early\n",
      "pc\n",
      "accommodation\n",
      "prof\n",
      "cloth\n",
      "dutch\n",
      "hr\n",
      "race\n",
      "size\n",
      "object\n",
      "review\n",
      "proud\n",
      "optional\n",
      "sake\n",
      "desire\n",
      "teacher\n",
      "concept\n",
      "postscript\n",
      "compuserve\n",
      "figure\n",
      "criminal\n",
      "grow\n",
      "virtual\n",
      "happy\n",
      "25\n",
      "apply\n",
      "rather\n",
      "central\n",
      "legally\n",
      "exact\n",
      "post\n",
      "generative\n",
      "volume\n",
      "satisfy\n",
      "discipline\n",
      "promotion\n",
      "trust\n",
      "lanse\n",
      "september\n",
      "methodology\n",
      "define\n",
      "moment\n",
      "on\n",
      "exceedingly\n",
      "dave\n",
      "japan\n",
      "consider\n",
      "consist\n",
      "196\n",
      "directory\n",
      "girlfriend\n",
      "amex\n",
      "illegal\n",
      "header\n",
      "countless\n",
      "fastest\n",
      "embark\n",
      "209\n",
      "implication\n",
      "jame\n",
      "wisely\n",
      "murkowskus\n",
      "profanity\n",
      "poorer\n",
      "catchy\n",
      "instructed\n",
      "unproductive\n",
      "recession\n",
      "billboard\n",
      "overflow\n",
      "mega\n",
      "flamer\n",
      "pic\n",
      "divorce\n",
      "downpayment\n",
      "newsgroup\n",
      "several\n",
      "utility\n",
      "149\n",
      "obligation\n",
      "oxford\n",
      "develop\n",
      "dr\n",
      "univ\n",
      "copy\n",
      "excellent\n",
      "peter\n",
      "up\n",
      "encourage\n",
      "install\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = [10, 100, 1000]\n",
    "for i,n in enumerate(N):\n",
    "    top_feature, drop_feature, name_feature = feature_selection(n, x_train)\n",
    "    print_feature(top_feature,name_feature)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes for feature generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_feature(ls_train, ls_test):\n",
    "    # Use the count vectorizer classes to get binary featrues\n",
    "    # Set parmeter 'binary' to True, all non zero counts are set to 1\n",
    "    count_vect_bf = CountVectorizer(binary=True)\n",
    "    \n",
    "    x_train_bf = count_vect_bf.fit_transform(ls_train.data)\n",
    "    x_test_bf  = count_vect_bf.transform(ls_test.data)\n",
    "    \n",
    "    # Still drop the unwanted features in training set\n",
    "    x_train_bf = x_train_bf[:, top_feature]\n",
    "    x_test_bf  = x_test_bf[:, top_feature]\n",
    "    \n",
    "    return x_train_bf, x_test_bf\n",
    "\n",
    "def term_frequency(ls_train, ls_test):\n",
    "    \n",
    "    x_train_tf = x_train\n",
    "    x_test_tf = x_test\n",
    "\n",
    "\n",
    "    # Still drop the unwanted features in training set\n",
    "    x_train_tf = x_train_tf[:, top_feature]\n",
    "    x_test_tf  = x_test_tf[:, top_feature]\n",
    "    \n",
    "    return x_train_tf, x_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_train = skd.load_files('./data/lingspam_public/lemm_stop/train');\n",
    "ls_test  = skd.load_files('./data/lingspam_public/lemm_stop/test');\n",
    "\n",
    "x_train_bf, x_test_bf = binary_feature(ls_train, ls_test)\n",
    "x_train_tf, x_test_tf = term_frequency(ls_train, ls_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes for applying classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_classifiers(x_train_bf, x_train_tf, x_test_tf, x_test_bf, ls_test):\n",
    "    \n",
    "    # Multinomial NB with TF features\n",
    "    mNomTF = sklearn.naive_bayes.MultinomialNB()\n",
    "    mNomTF.fit(x_train_tf,ls_train.target)\n",
    "    \n",
    "    # Multinomial NB with BF features\n",
    "    mNomBF = sklearn.naive_bayes.MultinomialNB()\n",
    "    mNomBF.fit(x_train_bf,ls_train.target)\n",
    "\n",
    "    #Bernoulli NB classifier with BF features\n",
    "    bNolNB = sklearn.naive_bayes.BernoulliNB()\n",
    "    bNolNB.fit(x_train_bf,ls_train.target)\n",
    "    \n",
    "    # Predict on testind data\n",
    "    y_predict_M_TF = mNomTF.predict(x_test_tf)\n",
    "    y_predict_M_BF = mNomBF.predict(x_test_bf)\n",
    "    y_predict_B_BF = bNolNB.predict(x_test_bf)\n",
    "    \n",
    "    return y_predict_B_BF, y_predict_M_BF, y_predict_M_TF, mNomTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_B_BF, y_predict_M_BF, y_predict_M_TF,mNomTF = spam_classifiers(x_train_bf, \n",
    "                                                                  x_train_tf, \n",
    "                                                                  x_test_tf, \n",
    "                                                                  x_test_bf, \n",
    "                                                                  ls_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes for calculate precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def pre_rec(yts, yhat):\n",
    "    precision, recall, f1,_ = precision_recall_fscore_support(yts,\n",
    "                                                              yhat,\n",
    "                                                              average='binary')\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Codes for lad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10, 100, 1000]\n",
    "pre_matrix = np.zeros([3,3])\n",
    "rec_matrix = np.zeros([3,3])\n",
    "\n",
    "for i,n in enumerate(N):\n",
    "    top_feature, drop_feature, name_feature = feature_selection(n, x_train)\n",
    "\n",
    "    x_train_bf, x_test_bf = binary_feature(ls_train, ls_test)\n",
    "    x_train_tf, x_test_tf = term_frequency(ls_train, ls_test)\n",
    "    \n",
    "    y_predict_B_BF, y_predict_M_BF, y_predict_M_TF,mNomTF = spam_classifiers(x_train_bf, \n",
    "                                                                x_train_tf, \n",
    "                                                                x_test_tf, \n",
    "                                                                x_test_bf, \n",
    "                                                                ls_test)\n",
    "    if (n==1000):\n",
    "        eval_feature_1000 = top_feature\n",
    "    \n",
    "\n",
    "    # Multinomial NB with TF\n",
    "    pre1,rec1 = pre_rec(ls_test.target,y_predict_M_TF)\n",
    "    \n",
    "    #Multinomial NB with BF\n",
    "    pre2,rec2 = pre_rec(ls_test.target,y_predict_M_BF)\n",
    "        \n",
    "    #Bernoulli NB classifier with BF\n",
    "    pre3,rec3 = pre_rec(ls_test.target,y_predict_B_BF)\n",
    "    \n",
    "    pre_matrix[:,i] = [pre1, pre2, pre3]\n",
    "    rec_matrix[:,i] = [rec1, rec2, rec3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write precision and recall matrices to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Number of Featrues</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial NB with TF</th>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial NB with BF</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB with BF</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Number of Featrues          10        100   1000\n",
       "Multinomial NB with TF  0.851852  0.959184   1.0\n",
       "Multinomial NB with BF  0.888889  0.977778   1.0\n",
       "Bernoulli NB with BF    0.869565  0.939394   1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = [str('Multinomial NB with TF'),\n",
    "              str('Multinomial NB with BF'), \n",
    "              str('Bernoulli NB with BF')]\n",
    "featurenumber = [10, 100, 1000]\n",
    "\n",
    "df_pre = pd.DataFrame(data = pre_matrix[0:,0:],\n",
    "                      index = classifier,\n",
    "                      columns = featurenumber)\n",
    "df_pre.columns.name = 'Number of Featrues' \n",
    "df_pre.name = 'Precision Table'\n",
    "\n",
    "with open('precisiontable.tex','w') as tf:\n",
    "    tf.write(df_pre.to_latex())\n",
    "    \n",
    "df_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Number of Featrues</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial NB with TF</th>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.938776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial NB with BF</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.938776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB with BF</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.612245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Number of Featrues          10        100       1000\n",
       "Multinomial NB with TF  0.938776  0.959184  0.938776\n",
       "Multinomial NB with BF  0.816327  0.897959  0.938776\n",
       "Bernoulli NB with BF    0.816327  0.632653  0.612245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec = pd.DataFrame(data = rec_matrix[0:,0:],\n",
    "                      index = classifier,\n",
    "                      columns = featurenumber)\n",
    "df_rec.columns.name = 'Number of Featrues' \n",
    "\n",
    "with open('recalltable.tex','w') as tf:\n",
    "    tf.write(df_rec.to_latex())\n",
    "    \n",
    "df_rec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Eval Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Import eval set\n",
    "eval_set = sklearn.datasets.load_files('./eval', shuffle=False)\n",
    "\n",
    "# Create Vocabulary from training set\n",
    "tf_transformer_new = TfidfTransformer(use_idf=False)\n",
    "x_train_tf = tf_transformer_new.fit_transform(x_train)\n",
    "x_train = count_vect.fit_transform(ls_train.data)\n",
    "\n",
    "# Transform eval data to term frequency\n",
    "eval_set_data = count_vect.transform(eval_set.data)\n",
    "\n",
    "# Select top 10 featrues\n",
    "eval_set_data = eval_set_data[:,top_feature]\n",
    "\n",
    "# Predict\n",
    "eval_set_predict = mNomTF.predict(eval_set_data)\n",
    "\n",
    "# Save the Result to Text file\n",
    "np.savetxt(\"./eval/results_new.txt\", eval_set_predict, newline=\"\\r\\n\", fmt=\"%d\")\n",
    "\n",
    "print(eval_set_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
