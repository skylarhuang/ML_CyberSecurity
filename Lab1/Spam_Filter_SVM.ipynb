{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets as skd\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "#Each sub-directory in the parent directory is assumed to contain documents from the same class\n",
    "#I pre-processed the part1 (fold1) and part2 (fold2) of the lingspam dataset to place spam emails in one folder \n",
    "#and legit emails in another; you should do the same for the entire dataset, either manually or via a script. \n",
    "ls_train = skd.load_files('./data/lingspam_public/lemm_stop/train');\n",
    "ls_test  = skd.load_files('./data/lingspam_public/lemm_stop/test');\n",
    "\n",
    "#The count vectorizer classes fit_transform function generates a vocoabulary that contains each unique term in the dataset\n",
    "#and outputs a sparse matrix tabulating term occurences\n",
    "count_vect = CountVectorizer()\n",
    "x_train = count_vect.fit_transform(ls_train.data)\n",
    "\n",
    "#Since the vocabulary has already been learned, use the transform function to transform the test data using the same vocab\n",
    "x_test = count_vect.transform(ls_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(N,x_train):\n",
    "    # Prepare\n",
    "    num_email   = x_train.shape[0]\n",
    "    num_feature = x_train.shape[1]\n",
    "\n",
    "    # Transport saved data from sparse matrix out\n",
    "    x_train_data = x_train.toarray()\n",
    "    x_train_ig   = np.zeros([num_feature])\n",
    "\n",
    "    for i in range(num_feature):\n",
    "        # Each colunm show the occurence of one feature in all emails\n",
    "        feature_vector = x_train_data[:,i]\n",
    "    \n",
    "        # Reshape \n",
    "        feature_vector = feature_vector.reshape([num_email])\n",
    "        \n",
    "        # Calculate ig for features \n",
    "        x_train_ig [i] = mutual_info_score(feature_vector, ls_train.target)\n",
    "        \n",
    "    x_train_ig_sort = np.argsort(-x_train_ig)\n",
    "\n",
    "    # Extract feature names\n",
    "    name_feature = count_vect.get_feature_names()\n",
    "\n",
    "    # Select N largest features' index\n",
    "    top_feature  = np.array(x_train_ig_sort[:N])\n",
    "    drop_feature = np.array(x_train_ig_sort[N:num_feature])\n",
    "    \n",
    "    return top_feature,drop_feature,name_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_feature(ls_train):\n",
    "    # Use the count vectorizer classes to get binary featrues\n",
    "    # Set parmeter 'binary' to True, all non zero counts are set to 1\n",
    "    count_vect_bf = CountVectorizer(binary=True)\n",
    "    \n",
    "    x_train_bf = count_vect_bf.fit_transform(ls_train.data)\n",
    "    x_test_bf  = count_vect_bf.transform(ls_test.data)\n",
    "    \n",
    "    # Still drop the unwanted features in training set\n",
    "    x_train_bf = x_train_bf[:, top_feature]\n",
    "    x_test_bf  = x_test_bf[:, top_feature]\n",
    "    \n",
    "    return x_train_bf, x_test_bf\n",
    "\n",
    "def term_frequency(ls_train):\n",
    "    tf_transformer = TfidfTransformer(use_idf=False)\n",
    "\n",
    "    x_train_tf = tf_transformer.fit_transform(x_train)\n",
    "    x_test_tf = tf_transformer.transform(x_test)\n",
    "\n",
    "    # Still drop the unwanted features in training set\n",
    "    x_train_tf = x_train_tf[:, top_feature]\n",
    "    x_test_tf  = x_test_tf[:, top_feature]\n",
    "    \n",
    "    return x_train_tf, x_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_rec(yts, yhat):\n",
    "    precision, recall, f1,_ = precision_recall_fscore_support(yts,\n",
    "                                                              yhat,\n",
    "                                                              average='binary')\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]\n",
      "\n",
      "Accuracy  = 0.979254\n",
      "Precision = 0.985364\n",
      "Recall    = 0.888575\n"
     ]
    }
   ],
   "source": [
    "# K-fold - split into 10 folds\n",
    "# 100 top features\n",
    "# Cross Validation\n",
    "# Using binary feature\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "top_feature, drop_feature, name_feature = feature_selection(100, x_train)\n",
    "\n",
    "x_train_bf, x_test_bf = binary_feature(ls_train)\n",
    "x_train_tf, x_test_tf = term_frequency(ls_train)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "x_train = count_vect.fit_transform(ls_train.data)\n",
    "\n",
    "nfold = 10\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "acc = []\n",
    "pre = []\n",
    "rec = []\n",
    "svc = SVC(kernel=\"sigmoid\", C=10, verbose=10)\n",
    "\n",
    "for train, test in kf.split(x_train_bf):\n",
    "    Xtr = x_train_bf[train,:]\n",
    "    Xts = x_train_bf[test,:]\n",
    "    ytr = ls_train.target[train]\n",
    "    yts = ls_train.target[test]\n",
    "    \n",
    "    svc.fit(Xtr,ytr)\n",
    "    yhat = svc.predict(Xts)\n",
    "    \n",
    "    acci = np.mean(yhat == yts)\n",
    "    prei,reci = pre_rec(yts,yhat)\n",
    "    \n",
    "    acc.append(acci)\n",
    "    pre.append(prei)\n",
    "    rec.append(reci)\n",
    "\n",
    "acc_mean = np.mean(acc)\n",
    "pre_mean = np.mean(pre)\n",
    "rec_mean = np.mean(rec)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Accuracy  = {0:f}'.format(acc_mean))\n",
    "print('Precision = {0:f}'.format(pre_mean))\n",
    "print('Recall    = {0:f}'.format(rec_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
